2024-09-03 22:30:53,123 - mmdet3d - INFO - Config:
seed = 0
deterministic = False
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
load_from = 'pretrained/lidar-only-det.pth'
resume_from = None
cudnn_benchmark = False
fp16 = dict(loss_scale=dict(growth_interval=2000))
max_epochs = 10
runner = dict(type='CustomEpochBasedRunner', max_epochs=10)
dataset_type = 'NuScenesDataset'
dataset_root = '/scratch/jmeng18/V2X-SIM/'
gt_paste_stop_epoch = -1
reduce_beams = 32
load_dim = 5
use_dim = 5
load_augmented = None
point_cloud_range = [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
voxel_size = [0.075, 0.075, 0.2]
image_size = [256, 704]
augment2d = dict(
    resize=[[0.38, 0.55], [0.48, 0.48]],
    rotate=[-5.4, 5.4],
    gridmask=dict(prob=0.0, fixed_prob=True))
augment3d = dict(
    scale=[0.9, 1.1], rotate=[-0.78539816, 0.78539816], translate=0.5)
object_classes = ['car']
map_classes = [
    'drivable_area', 'ped_crossing', 'walkway', 'stop_line', 'carpark_area'
]
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        reduce_beams=32,
        load_augmented=None),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=0,
        load_dim=5,
        use_dim=5,
        reduce_beams=32,
        pad_empty_sweeps=True,
        remove_close=True,
        load_augmented=None),
    dict(
        type='LoadAnnotations3D',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False),
    dict(
        type='ObjectPaste',
        stop_epoch=-1,
        db_sampler=dict(
            dataset_root='/scratch/jmeng18/V2X-SIM/',
            info_path='/scratch/jmeng18/V2X-SIM/nuscenes_dbinfos_train.pkl',
            rate=1.0,
            prepare=dict(
                filter_by_difficulty=[-1], filter_by_min_points=dict(car=5)),
            classes=['car'],
            sample_groups=dict(car=2),
            points_loader=dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                reduce_beams=32))),
    dict(
        type='ImageAug3D',
        final_dim=[256, 704],
        resize_lim=[0.38, 0.55],
        bot_pct_lim=[0.0, 0.0],
        rot_lim=[-5.4, 5.4],
        rand_flip=True,
        is_train=True),
    dict(
        type='GlobalRotScaleTrans',
        resize_lim=[0.9, 1.1],
        rot_lim=[-0.78539816, 0.78539816],
        trans_lim=0.5,
        is_train=True),
    dict(type='RandomFlip3D'),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
    dict(type='ObjectNameFilter', classes=['car']),
    dict(
        type='ImageNormalize',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(
        type='GridMask',
        use_h=True,
        use_w=True,
        max_epoch=10,
        rotate=1,
        offset=False,
        ratio=0.5,
        mode=1,
        prob=0.0,
        fixed_prob=True),
    dict(type='PointShuffle'),
    dict(type='DefaultFormatBundle3D', classes=['car']),
    dict(
        type='Collect3D',
        keys=['img', 'points', 'gt_bboxes_3d', 'gt_labels_3d'],
        meta_keys=[
            'camera_intrinsics', 'camera2ego', 'lidar2ego', 'lidar2camera',
            'lidar2image', 'camera2lidar', 'img_aug_matrix', 'lidar_aug_matrix'
        ]),
    dict(type='GTDepth', keyframe_only=True)
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        reduce_beams=32,
        load_augmented=None),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        load_dim=5,
        use_dim=5,
        reduce_beams=32,
        pad_empty_sweeps=True,
        remove_close=True,
        load_augmented=None),
    dict(
        type='LoadAnnotations3D',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=False),
    dict(
        type='ImageAug3D',
        final_dim=[256, 704],
        resize_lim=[0.48, 0.48],
        bot_pct_lim=[0.0, 0.0],
        rot_lim=[0.0, 0.0],
        rand_flip=False,
        is_train=False),
    dict(
        type='GlobalRotScaleTrans',
        resize_lim=[1.0, 1.0],
        rot_lim=[0.0, 0.0],
        trans_lim=0.0,
        is_train=False),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
    dict(
        type='ImageNormalize',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(type='DefaultFormatBundle3D', classes=['car']),
    dict(
        type='Collect3D',
        keys=['img', 'points', 'gt_bboxes_3d', 'gt_labels_3d'],
        meta_keys=[
            'camera_intrinsics', 'camera2ego', 'lidar2ego', 'lidar2camera',
            'lidar2image', 'camera2lidar', 'img_aug_matrix', 'lidar_aug_matrix'
        ]),
    dict(type='GTDepth', keyframe_only=True)
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=1,
    train=dict(
        type='CBGSDataset',
        dataset=dict(
            type='NuScenesDataset',
            dataset_root='/scratch/jmeng18/V2X-SIM/',
            ann_file='/scratch/jmeng18/V2X-SIM/nuscenes_infos_train.pkl',
            pipeline=[
                dict(type='LoadMultiViewImageFromFiles', to_float32=True),
                dict(
                    type='LoadPointsFromFile',
                    coord_type='LIDAR',
                    load_dim=5,
                    use_dim=5,
                    reduce_beams=32,
                    load_augmented=None),
                dict(
                    type='LoadPointsFromMultiSweeps',
                    sweeps_num=0,
                    load_dim=5,
                    use_dim=5,
                    reduce_beams=32,
                    pad_empty_sweeps=True,
                    remove_close=True,
                    load_augmented=None),
                dict(
                    type='LoadAnnotations3D',
                    with_bbox_3d=True,
                    with_label_3d=True,
                    with_attr_label=False),
                dict(
                    type='ObjectPaste',
                    stop_epoch=-1,
                    db_sampler=dict(
                        dataset_root='/scratch/jmeng18/V2X-SIM/',
                        info_path=
                        '/scratch/jmeng18/V2X-SIM/nuscenes_dbinfos_train.pkl',
                        rate=1.0,
                        prepare=dict(
                            filter_by_difficulty=[-1],
                            filter_by_min_points=dict(car=5)),
                        classes=['car'],
                        sample_groups=dict(car=2),
                        points_loader=dict(
                            type='LoadPointsFromFile',
                            coord_type='LIDAR',
                            load_dim=5,
                            use_dim=5,
                            reduce_beams=32))),
                dict(
                    type='ImageAug3D',
                    final_dim=[256, 704],
                    resize_lim=[0.38, 0.55],
                    bot_pct_lim=[0.0, 0.0],
                    rot_lim=[-5.4, 5.4],
                    rand_flip=True,
                    is_train=True),
                dict(
                    type='GlobalRotScaleTrans',
                    resize_lim=[0.9, 1.1],
                    rot_lim=[-0.78539816, 0.78539816],
                    trans_lim=0.5,
                    is_train=True),
                dict(type='RandomFlip3D'),
                dict(
                    type='PointsRangeFilter',
                    point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
                dict(
                    type='ObjectRangeFilter',
                    point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
                dict(type='ObjectNameFilter', classes=['car']),
                dict(
                    type='ImageNormalize',
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]),
                dict(
                    type='GridMask',
                    use_h=True,
                    use_w=True,
                    max_epoch=10,
                    rotate=1,
                    offset=False,
                    ratio=0.5,
                    mode=1,
                    prob=0.0,
                    fixed_prob=True),
                dict(type='PointShuffle'),
                dict(type='DefaultFormatBundle3D', classes=['car']),
                dict(
                    type='Collect3D',
                    keys=['img', 'points', 'gt_bboxes_3d', 'gt_labels_3d'],
                    meta_keys=[
                        'camera_intrinsics', 'camera2ego', 'lidar2ego',
                        'lidar2camera', 'lidar2image', 'camera2lidar',
                        'img_aug_matrix', 'lidar_aug_matrix'
                    ]),
                dict(type='GTDepth', keyframe_only=True)
            ],
            object_classes=['car'],
            map_classes=[
                'drivable_area', 'ped_crossing', 'walkway', 'stop_line',
                'carpark_area'
            ],
            modality=dict(
                use_lidar=True,
                use_camera=True,
                use_radar=False,
                use_map=False,
                use_external=False),
            test_mode=False,
            use_valid_flag=True,
            box_type_3d='LiDAR')),
    val=dict(
        type='NuScenesDataset',
        dataset_root='/scratch/jmeng18/V2X-SIM/',
        ann_file='/scratch/jmeng18/V2X-SIM/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                reduce_beams=32,
                load_augmented=None),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                load_dim=5,
                use_dim=5,
                reduce_beams=32,
                pad_empty_sweeps=True,
                remove_close=True,
                load_augmented=None),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False),
            dict(
                type='ImageAug3D',
                final_dim=[256, 704],
                resize_lim=[0.48, 0.48],
                bot_pct_lim=[0.0, 0.0],
                rot_lim=[0.0, 0.0],
                rand_flip=False,
                is_train=False),
            dict(
                type='GlobalRotScaleTrans',
                resize_lim=[1.0, 1.0],
                rot_lim=[0.0, 0.0],
                trans_lim=0.0,
                is_train=False),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
            dict(
                type='ImageNormalize',
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]),
            dict(type='DefaultFormatBundle3D', classes=['car']),
            dict(
                type='Collect3D',
                keys=['img', 'points', 'gt_bboxes_3d', 'gt_labels_3d'],
                meta_keys=[
                    'camera_intrinsics', 'camera2ego', 'lidar2ego',
                    'lidar2camera', 'lidar2image', 'camera2lidar',
                    'img_aug_matrix', 'lidar_aug_matrix'
                ]),
            dict(type='GTDepth', keyframe_only=True)
        ],
        object_classes=['car'],
        map_classes=[
            'drivable_area', 'ped_crossing', 'walkway', 'stop_line',
            'carpark_area'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        dataset_root='/scratch/jmeng18/V2X-SIM/',
        ann_file='/scratch/jmeng18/V2X-SIM/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                reduce_beams=32,
                load_augmented=None),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                load_dim=5,
                use_dim=5,
                reduce_beams=32,
                pad_empty_sweeps=True,
                remove_close=True,
                load_augmented=None),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=False),
            dict(
                type='ImageAug3D',
                final_dim=[256, 704],
                resize_lim=[0.48, 0.48],
                bot_pct_lim=[0.0, 0.0],
                rot_lim=[0.0, 0.0],
                rand_flip=False,
                is_train=False),
            dict(
                type='GlobalRotScaleTrans',
                resize_lim=[1.0, 1.0],
                rot_lim=[0.0, 0.0],
                trans_lim=0.0,
                is_train=False),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
            dict(
                type='ImageNormalize',
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]),
            dict(type='DefaultFormatBundle3D', classes=['car']),
            dict(
                type='Collect3D',
                keys=['img', 'points', 'gt_bboxes_3d', 'gt_labels_3d'],
                meta_keys=[
                    'camera_intrinsics', 'camera2ego', 'lidar2ego',
                    'lidar2camera', 'lidar2image', 'camera2lidar',
                    'img_aug_matrix', 'lidar_aug_matrix'
                ]),
            dict(type='GTDepth', keyframe_only=True)
        ],
        object_classes=['car'],
        map_classes=[
            'drivable_area', 'ped_crossing', 'walkway', 'stop_line',
            'carpark_area'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(type='LoadMultiViewImageFromFiles', to_float32=True),
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            reduce_beams=32,
            load_augmented=None),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=9,
            load_dim=5,
            use_dim=5,
            reduce_beams=32,
            pad_empty_sweeps=True,
            remove_close=True,
            load_augmented=None),
        dict(
            type='LoadAnnotations3D',
            with_bbox_3d=True,
            with_label_3d=True,
            with_attr_label=False),
        dict(
            type='ImageAug3D',
            final_dim=[256, 704],
            resize_lim=[0.48, 0.48],
            bot_pct_lim=[0.0, 0.0],
            rot_lim=[0.0, 0.0],
            rand_flip=False,
            is_train=False),
        dict(
            type='GlobalRotScaleTrans',
            resize_lim=[1.0, 1.0],
            rot_lim=[0.0, 0.0],
            trans_lim=0.0,
            is_train=False),
        dict(
            type='PointsRangeFilter',
            point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]),
        dict(
            type='ImageNormalize',
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]),
        dict(type='DefaultFormatBundle3D', classes=['car']),
        dict(
            type='Collect3D',
            keys=['img', 'points', 'gt_bboxes_3d', 'gt_labels_3d'],
            meta_keys=[
                'camera_intrinsics', 'camera2ego', 'lidar2ego', 'lidar2camera',
                'lidar2image', 'camera2lidar', 'img_aug_matrix',
                'lidar_aug_matrix'
            ]),
        dict(type='GTDepth', keyframe_only=True)
    ])
radar_sweeps = 6
radar_max_points = 2500
radar_use_dims = [
    0, 1, 2, 5, 8, 9, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,
    32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,
    51, 52, 53, 54, 55, 56
]
radar_compensate_velocity = True
radar_filtering = 'none'
radar_voxel_size = [0.8, 0.8, 8]
radar_jitter = 0
radar_normalize = False
model = dict(
    type='BEVFusion',
    encoders=dict(
        camera=dict(
            neck=dict(
                type='GeneralizedLSSFPN',
                in_channels=[192, 384, 768],
                out_channels=256,
                start_level=0,
                num_outs=3,
                norm_cfg=dict(type='BN2d', requires_grad=True),
                act_cfg=dict(type='ReLU', inplace=True),
                upsample_cfg=dict(mode='bilinear', align_corners=False)),
            vtransform=dict(
                type='DepthLSSTransform',
                in_channels=256,
                out_channels=80,
                image_size=[256, 704],
                feature_size=[32, 88],
                xbound=[-54.0, 54.0, 0.3],
                ybound=[-54.0, 54.0, 0.3],
                zbound=[-10.0, 10.0, 20.0],
                dbound=[1.0, 60.0, 0.5],
                downsample=2),
            backbone=dict(
                type='SwinTransformer',
                embed_dims=96,
                depths=[2, 2, 6, 2],
                num_heads=[3, 6, 12, 24],
                window_size=7,
                mlp_ratio=4,
                qkv_bias=True,
                qk_scale=None,
                drop_rate=0.0,
                attn_drop_rate=0.0,
                drop_path_rate=0.2,
                patch_norm=True,
                out_indices=[1, 2, 3],
                with_cp=False,
                convert_weights=True,
                init_cfg=dict(
                    type='Pretrained',
                    checkpoint='pretrained/swint-nuimages-pretrained.pth'))),
        lidar=dict(
            voxelize=dict(
                max_num_points=10,
                point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0],
                voxel_size=[0.075, 0.075, 0.2],
                max_voxels=[120000, 160000]),
            backbone=dict(
                type='SparseEncoder',
                in_channels=5,
                sparse_shape=[1440, 1440, 41],
                output_channels=128,
                order=['conv', 'norm', 'act'],
                encoder_channels=[[16, 16, 32], [32, 32, 64], [64, 64, 128],
                                  [128, 128]],
                encoder_paddings=[[0, 0, 1], [0, 0, 1], [0, 0, [1, 1, 0]],
                                  [0, 0]],
                block_type='basicblock')),
        infra=dict(
            neck=dict(
                type='GeneralizedLSSFPN',
                in_channels=[192, 384, 768],
                out_channels=256,
                start_level=0,
                num_outs=3,
                norm_cfg=dict(type='BN2d', requires_grad=True),
                act_cfg=dict(type='ReLU', inplace=True),
                upsample_cfg=dict(mode='bilinear', align_corners=False)),
            vtransform=dict(
                type='DepthLSSTransform',
                in_channels=256,
                out_channels=80,
                image_size=[256, 704],
                feature_size=[32, 88],
                xbound=[-54.0, 54.0, 0.3],
                ybound=[-54.0, 54.0, 0.3],
                zbound=[-10.0, 10.0, 20.0],
                dbound=[1.0, 60.0, 0.5],
                downsample=2),
            backbone=dict(
                type='SwinTransformer',
                embed_dims=96,
                depths=[2, 2, 6, 2],
                num_heads=[3, 6, 12, 24],
                window_size=7,
                mlp_ratio=4,
                qkv_bias=True,
                qk_scale=None,
                drop_rate=0.0,
                attn_drop_rate=0.0,
                drop_path_rate=0.2,
                patch_norm=True,
                out_indices=[1, 2, 3],
                with_cp=False,
                convert_weights=True,
                init_cfg=dict(
                    type='Pretrained',
                    checkpoint=
                    'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'
                )))),
    fuser=dict(type='ConvFuser', in_channels=[80, 256, 80], out_channels=256),
    heads=dict(
        map=None,
        object=dict(
            type='TransFusionHead',
            num_proposals=200,
            auxiliary=True,
            in_channels=512,
            hidden_channel=128,
            num_classes=1,
            num_decoder_layers=1,
            num_heads=8,
            nms_kernel_size=3,
            ffn_channel=256,
            dropout=0.1,
            bn_momentum=0.1,
            activation='relu',
            train_cfg=dict(
                dataset='nuScenes',
                point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0],
                grid_size=[1440, 1440, 41],
                voxel_size=[0.075, 0.075, 0.2],
                out_size_factor=8,
                gaussian_overlap=0.1,
                min_radius=2,
                pos_weight=-1,
                code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
                assigner=dict(
                    type='HungarianAssigner3D',
                    iou_calculator=dict(
                        type='BboxOverlaps3D', coordinate='lidar'),
                    cls_cost=dict(
                        type='FocalLossCost',
                        gamma=2.0,
                        alpha=0.25,
                        weight=0.15),
                    reg_cost=dict(type='BBoxBEVL1Cost', weight=0.25),
                    iou_cost=dict(type='IoU3DCost', weight=0.25))),
            test_cfg=dict(
                dataset='nuScenes',
                grid_size=[1440, 1440, 41],
                out_size_factor=8,
                voxel_size=[0.075, 0.075],
                pc_range=[-54.0, -54.0],
                nms_type=None),
            common_heads=dict(
                center=[2, 2],
                height=[1, 2],
                dim=[3, 2],
                rot=[2, 2],
                vel=[2, 2]),
            bbox_coder=dict(
                type='TransFusionBBoxCoder',
                pc_range=[-54.0, -54.0],
                post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
                score_threshold=0.0,
                out_size_factor=8,
                voxel_size=[0.075, 0.075]),
            loss_cls=dict(
                type='FocalLoss',
                use_sigmoid=True,
                gamma=2.0,
                alpha=0.25,
                reduction='mean',
                loss_weight=1.0),
            loss_heatmap=dict(
                type='GaussianFocalLoss', reduction='mean', loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', reduction='mean',
                           loss_weight=0.25))),
    decoder=dict(
        backbone=dict(
            type='SECOND',
            in_channels=256,
            out_channels=[128, 256],
            layer_nums=[5, 5],
            layer_strides=[1, 2],
            norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
            conv_cfg=dict(type='Conv2d', bias=False)),
        neck=dict(
            type='SECONDFPN',
            in_channels=[128, 256],
            out_channels=[256, 256],
            upsample_strides=[1, 2],
            norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
            upsample_cfg=dict(type='deconv', bias=False),
            use_conv_for_no_stride=True)))
optimizer = dict(type='AdamW', lr=0.0002, weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.33333333,
    min_lr_ratio=1e-07)
momentum_config = dict(policy='cyclic')
run_dir = 'train_result_infra_short_full'

2024-09-03 22:30:53,124 - mmdet3d - INFO - Set random seed to 0, deterministic mode: False
2024-09-03 22:30:59,698 - mmdet3d - INFO - load 349483 car database infos
2024-09-03 22:30:59,698 - mmdet3d - INFO - load 157002 bicycle database infos
2024-09-03 22:30:59,698 - mmdet3d - INFO - load 40210 motorcycle database infos
2024-09-03 22:30:59,698 - mmdet3d - INFO - load 16943 vehicle.emergency.police database infos
2024-09-03 22:31:00,523 - mmdet3d - INFO - After filter database:
2024-09-03 22:31:00,536 - mmdet3d - INFO - load 170227 car database infos
2024-09-03 22:31:00,537 - mmdet3d - INFO - load 157002 bicycle database infos
2024-09-03 22:31:00,538 - mmdet3d - INFO - load 40210 motorcycle database infos
2024-09-03 22:31:00,538 - mmdet3d - INFO - load 16943 vehicle.emergency.police database infos
2024-09-03 22:31:01,356 - mmdet3d - INFO - Model:
BEVFusion(
  (encoders): ModuleDict(
    (camera): ModuleDict(
      (backbone): SwinTransformer(
        (patch_embed): PatchEmbed(
          (adap_padding): AdaptivePadding()
          (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        )
        (drop_after_pos): Dropout(p=0.0, inplace=False)
        (stages): ModuleList(
          (0): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=96, out_features=288, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=96, out_features=96, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=96, out_features=384, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=384, out_features=96, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=96, out_features=288, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=96, out_features=96, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=96, out_features=384, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=384, out_features=96, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
            (downsample): PatchMerging(
              (adap_padding): AdaptivePadding()
              (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (reduction): Linear(in_features=384, out_features=192, bias=False)
            )
          )
          (1): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=192, out_features=576, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=192, out_features=192, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=192, out_features=768, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=768, out_features=192, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=192, out_features=576, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=192, out_features=192, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=192, out_features=768, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=768, out_features=192, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
            (downsample): PatchMerging(
              (adap_padding): AdaptivePadding()
              (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (reduction): Linear(in_features=768, out_features=384, bias=False)
            )
          )
          (2): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (2): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (3): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (4): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (5): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
            (downsample): PatchMerging(
              (adap_padding): AdaptivePadding()
              (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (reduction): Linear(in_features=1536, out_features=768, bias=False)
            )
          )
          (3): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=768, out_features=2304, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=768, out_features=768, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=768, out_features=3072, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=3072, out_features=768, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=768, out_features=2304, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=768, out_features=768, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=768, out_features=3072, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=3072, out_features=768, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
          )
        )
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      init_cfg={'type': 'Pretrained', 'checkpoint': 'pretrained/swint-nuimages-pretrained.pth'}
      (neck): GeneralizedLSSFPN(
        (lateral_convs): ModuleList(
          (0): ConvModule(
            (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): ConvModule(
            (conv): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
        (fpn_convs): ModuleList(
          (0): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (vtransform): DepthLSSTransform(
        (dtransform): Sequential(
          (0): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(8, 32, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (8): ReLU(inplace=True)
        )
        (depthnet): Sequential(
          (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(256, 198, kernel_size=(1, 1), stride=(1, 1))
        )
        (downsample): Sequential(
          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (8): ReLU(inplace=True)
        )
      )
    )
    (lidar): ModuleDict(
      (voxelize): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54.0, -54.0, -5.0, 54.0, 54.0, 3.0], max_num_points=10, max_voxels=(120000, 160000), deterministic=True)
      (backbone): SparseEncoder(
        (conv_input): SparseSequential(
          (0): SubMConv3d()
          (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (encoder_layers): SparseSequential(
          (encoder_layer1): SparseSequential(
            (0): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (1): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (2): SparseSequential(
              (0): SparseConv3d()
              (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (encoder_layer2): SparseSequential(
            (0): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (1): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (2): SparseSequential(
              (0): SparseConv3d()
              (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (encoder_layer3): SparseSequential(
            (0): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (1): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (2): SparseSequential(
              (0): SparseConv3d()
              (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (encoder_layer4): SparseSequential(
            (0): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
            (1): SparseBasicBlock(
              (conv1): SubMConv3d()
              (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (conv2): SubMConv3d()
              (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu): ReLU(inplace=True)
            )
          )
        )
        (conv_out): SparseSequential(
          (0): SparseConv3d()
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
    )
    (infra): ModuleDict(
      (backbone): SwinTransformer(
        (patch_embed): PatchEmbed(
          (adap_padding): AdaptivePadding()
          (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        )
        (drop_after_pos): Dropout(p=0.0, inplace=False)
        (stages): ModuleList(
          (0): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=96, out_features=288, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=96, out_features=96, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=96, out_features=384, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=384, out_features=96, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=96, out_features=288, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=96, out_features=96, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=96, out_features=384, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=384, out_features=96, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
            (downsample): PatchMerging(
              (adap_padding): AdaptivePadding()
              (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (reduction): Linear(in_features=384, out_features=192, bias=False)
            )
          )
          (1): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=192, out_features=576, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=192, out_features=192, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=192, out_features=768, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=768, out_features=192, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=192, out_features=576, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=192, out_features=192, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=192, out_features=768, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=768, out_features=192, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
            (downsample): PatchMerging(
              (adap_padding): AdaptivePadding()
              (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (reduction): Linear(in_features=768, out_features=384, bias=False)
            )
          )
          (2): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (2): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (3): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (4): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (5): SwinBlock(
                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=384, out_features=1152, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=384, out_features=384, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=384, out_features=1536, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=1536, out_features=384, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
            (downsample): PatchMerging(
              (adap_padding): AdaptivePadding()
              (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
              (reduction): Linear(in_features=1536, out_features=768, bias=False)
            )
          )
          (3): SwinBlockSequence(
            (blocks): ModuleList(
              (0): SwinBlock(
                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=768, out_features=2304, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=768, out_features=768, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=768, out_features=3072, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=3072, out_features=768, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
              (1): SwinBlock(
                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attn): ShiftWindowMSA(
                  (w_msa): WindowMSA(
                    (qkv): Linear(in_features=768, out_features=2304, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=768, out_features=768, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop): DropPath()
                )
                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (ffn): FFN(
                  (activate): GELU()
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=768, out_features=3072, bias=True)
                      (1): GELU()
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=3072, out_features=768, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): DropPath()
                )
              )
            )
          )
        )
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      init_cfg={'type': 'Pretrained', 'checkpoint': 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'}
      (neck): GeneralizedLSSFPN(
        (lateral_convs): ModuleList(
          (0): ConvModule(
            (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): ConvModule(
            (conv): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
        (fpn_convs): ModuleList(
          (0): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
          (1): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
      )
      (vtransform): DepthLSSTransform(
        (dtransform): Sequential(
          (0): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1))
          (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(8, 32, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
          (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (8): ReLU(inplace=True)
        )
        (depthnet): Sequential(
          (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(256, 198, kernel_size=(1, 1), stride=(1, 1))
        )
        (downsample): Sequential(
          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (8): ReLU(inplace=True)
        )
      )
    )
  )
  (fuser): ConvFuser(
    (0): Conv2d(416, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (decoder): ModuleDict(
    (backbone): SECOND(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (8): ReLU(inplace=True)
          (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (11): ReLU(inplace=True)
          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (14): ReLU(inplace=True)
          (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (17): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
          (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (8): ReLU(inplace=True)
          (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (11): ReLU(inplace=True)
          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (14): ReLU(inplace=True)
          (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (17): ReLU(inplace=True)
        )
      )
    )
    init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
    (neck): SECONDFPN(
      (deblocks): ModuleList(
        (0): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (1): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
    )
    init_cfg=[{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]
  )
  (heads): ModuleDict(
    (object): TransFusionHead(
      (loss_cls): FocalLoss()
      (loss_bbox): L1Loss()
      (loss_iou): VarifocalLoss()
      (loss_heatmap): GaussianFocalLoss()
      (shared_conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (heatmap_head): Sequential(
        (0): ConvModule(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
        (1): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (class_encoding): Conv1d(1, 128, kernel_size=(1,), stride=(1,))
      (decoder): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=256, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=256, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
          (self_posembed): PositionEmbeddingLearned(
            (position_embedding_head): Sequential(
              (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
            )
          )
          (cross_posembed): PositionEmbeddingLearned(
            (position_embedding_head): Sequential(
              (0): Conv1d(2, 128, kernel_size=(1,), stride=(1,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
            )
          )
        )
      )
      (prediction_heads): ModuleList(
        (0): FFN(
          (center): Sequential(
            (0): ConvModule(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv1d(64, 3, kernel_size=(1,), stride=(1,))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv1d(64, 2, kernel_size=(1,), stride=(1,))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
          )
        )
      )
    )
  )
)
2024-09-03 22:31:04,242 - mmdet3d - INFO - load checkpoint from local path: pretrained/lidar-only-det.pth
2024-09-03 22:31:04,306 - mmdet3d - WARNING - The model and loaded state dict do not match exactly

size mismatch for heads.object.heatmap_head.1.weight: copying a param with shape torch.Size([10, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 128, 3, 3]).
size mismatch for heads.object.heatmap_head.1.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([1]).
size mismatch for heads.object.class_encoding.weight: copying a param with shape torch.Size([128, 10, 1]) from checkpoint, the shape in current model is torch.Size([128, 1, 1]).
size mismatch for heads.object.prediction_heads.0.heatmap.1.weight: copying a param with shape torch.Size([10, 64, 1]) from checkpoint, the shape in current model is torch.Size([1, 64, 1]).
size mismatch for heads.object.prediction_heads.0.heatmap.1.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([1]).
missing keys in source state_dict: encoders.camera.backbone.patch_embed.projection.weight, encoders.camera.backbone.patch_embed.projection.bias, encoders.camera.backbone.patch_embed.norm.weight, encoders.camera.backbone.patch_embed.norm.bias, encoders.camera.backbone.stages.0.blocks.0.norm1.weight, encoders.camera.backbone.stages.0.blocks.0.norm1.bias, encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.proj.weight, encoders.camera.backbone.stages.0.blocks.0.attn.w_msa.proj.bias, encoders.camera.backbone.stages.0.blocks.0.norm2.weight, encoders.camera.backbone.stages.0.blocks.0.norm2.bias, encoders.camera.backbone.stages.0.blocks.0.ffn.layers.0.0.weight, encoders.camera.backbone.stages.0.blocks.0.ffn.layers.0.0.bias, encoders.camera.backbone.stages.0.blocks.0.ffn.layers.1.weight, encoders.camera.backbone.stages.0.blocks.0.ffn.layers.1.bias, encoders.camera.backbone.stages.0.blocks.1.norm1.weight, encoders.camera.backbone.stages.0.blocks.1.norm1.bias, encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.proj.weight, encoders.camera.backbone.stages.0.blocks.1.attn.w_msa.proj.bias, encoders.camera.backbone.stages.0.blocks.1.norm2.weight, encoders.camera.backbone.stages.0.blocks.1.norm2.bias, encoders.camera.backbone.stages.0.blocks.1.ffn.layers.0.0.weight, encoders.camera.backbone.stages.0.blocks.1.ffn.layers.0.0.bias, encoders.camera.backbone.stages.0.blocks.1.ffn.layers.1.weight, encoders.camera.backbone.stages.0.blocks.1.ffn.layers.1.bias, encoders.camera.backbone.stages.0.downsample.norm.weight, encoders.camera.backbone.stages.0.downsample.norm.bias, encoders.camera.backbone.stages.0.downsample.reduction.weight, encoders.camera.backbone.stages.1.blocks.0.norm1.weight, encoders.camera.backbone.stages.1.blocks.0.norm1.bias, encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.proj.weight, encoders.camera.backbone.stages.1.blocks.0.attn.w_msa.proj.bias, encoders.camera.backbone.stages.1.blocks.0.norm2.weight, encoders.camera.backbone.stages.1.blocks.0.norm2.bias, encoders.camera.backbone.stages.1.blocks.0.ffn.layers.0.0.weight, encoders.camera.backbone.stages.1.blocks.0.ffn.layers.0.0.bias, encoders.camera.backbone.stages.1.blocks.0.ffn.layers.1.weight, encoders.camera.backbone.stages.1.blocks.0.ffn.layers.1.bias, encoders.camera.backbone.stages.1.blocks.1.norm1.weight, encoders.camera.backbone.stages.1.blocks.1.norm1.bias, encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.proj.weight, encoders.camera.backbone.stages.1.blocks.1.attn.w_msa.proj.bias, encoders.camera.backbone.stages.1.blocks.1.norm2.weight, encoders.camera.backbone.stages.1.blocks.1.norm2.bias, encoders.camera.backbone.stages.1.blocks.1.ffn.layers.0.0.weight, encoders.camera.backbone.stages.1.blocks.1.ffn.layers.0.0.bias, encoders.camera.backbone.stages.1.blocks.1.ffn.layers.1.weight, encoders.camera.backbone.stages.1.blocks.1.ffn.layers.1.bias, encoders.camera.backbone.stages.1.downsample.norm.weight, encoders.camera.backbone.stages.1.downsample.norm.bias, encoders.camera.backbone.stages.1.downsample.reduction.weight, encoders.camera.backbone.stages.2.blocks.0.norm1.weight, encoders.camera.backbone.stages.2.blocks.0.norm1.bias, encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.proj.weight, encoders.camera.backbone.stages.2.blocks.0.attn.w_msa.proj.bias, encoders.camera.backbone.stages.2.blocks.0.norm2.weight, encoders.camera.backbone.stages.2.blocks.0.norm2.bias, encoders.camera.backbone.stages.2.blocks.0.ffn.layers.0.0.weight, encoders.camera.backbone.stages.2.blocks.0.ffn.layers.0.0.bias, encoders.camera.backbone.stages.2.blocks.0.ffn.layers.1.weight, encoders.camera.backbone.stages.2.blocks.0.ffn.layers.1.bias, encoders.camera.backbone.stages.2.blocks.1.norm1.weight, encoders.camera.backbone.stages.2.blocks.1.norm1.bias, encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.proj.weight, encoders.camera.backbone.stages.2.blocks.1.attn.w_msa.proj.bias, encoders.camera.backbone.stages.2.blocks.1.norm2.weight, encoders.camera.backbone.stages.2.blocks.1.norm2.bias, encoders.camera.backbone.stages.2.blocks.1.ffn.layers.0.0.weight, encoders.camera.backbone.stages.2.blocks.1.ffn.layers.0.0.bias, encoders.camera.backbone.stages.2.blocks.1.ffn.layers.1.weight, encoders.camera.backbone.stages.2.blocks.1.ffn.layers.1.bias, encoders.camera.backbone.stages.2.blocks.2.norm1.weight, encoders.camera.backbone.stages.2.blocks.2.norm1.bias, encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.proj.weight, encoders.camera.backbone.stages.2.blocks.2.attn.w_msa.proj.bias, encoders.camera.backbone.stages.2.blocks.2.norm2.weight, encoders.camera.backbone.stages.2.blocks.2.norm2.bias, encoders.camera.backbone.stages.2.blocks.2.ffn.layers.0.0.weight, encoders.camera.backbone.stages.2.blocks.2.ffn.layers.0.0.bias, encoders.camera.backbone.stages.2.blocks.2.ffn.layers.1.weight, encoders.camera.backbone.stages.2.blocks.2.ffn.layers.1.bias, encoders.camera.backbone.stages.2.blocks.3.norm1.weight, encoders.camera.backbone.stages.2.blocks.3.norm1.bias, encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.proj.weight, encoders.camera.backbone.stages.2.blocks.3.attn.w_msa.proj.bias, encoders.camera.backbone.stages.2.blocks.3.norm2.weight, encoders.camera.backbone.stages.2.blocks.3.norm2.bias, encoders.camera.backbone.stages.2.blocks.3.ffn.layers.0.0.weight, encoders.camera.backbone.stages.2.blocks.3.ffn.layers.0.0.bias, encoders.camera.backbone.stages.2.blocks.3.ffn.layers.1.weight, encoders.camera.backbone.stages.2.blocks.3.ffn.layers.1.bias, encoders.camera.backbone.stages.2.blocks.4.norm1.weight, encoders.camera.backbone.stages.2.blocks.4.norm1.bias, encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.proj.weight, encoders.camera.backbone.stages.2.blocks.4.attn.w_msa.proj.bias, encoders.camera.backbone.stages.2.blocks.4.norm2.weight, encoders.camera.backbone.stages.2.blocks.4.norm2.bias, encoders.camera.backbone.stages.2.blocks.4.ffn.layers.0.0.weight, encoders.camera.backbone.stages.2.blocks.4.ffn.layers.0.0.bias, encoders.camera.backbone.stages.2.blocks.4.ffn.layers.1.weight, encoders.camera.backbone.stages.2.blocks.4.ffn.layers.1.bias, encoders.camera.backbone.stages.2.blocks.5.norm1.weight, encoders.camera.backbone.stages.2.blocks.5.norm1.bias, encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.proj.weight, encoders.camera.backbone.stages.2.blocks.5.attn.w_msa.proj.bias, encoders.camera.backbone.stages.2.blocks.5.norm2.weight, encoders.camera.backbone.stages.2.blocks.5.norm2.bias, encoders.camera.backbone.stages.2.blocks.5.ffn.layers.0.0.weight, encoders.camera.backbone.stages.2.blocks.5.ffn.layers.0.0.bias, encoders.camera.backbone.stages.2.blocks.5.ffn.layers.1.weight, encoders.camera.backbone.stages.2.blocks.5.ffn.layers.1.bias, encoders.camera.backbone.stages.2.downsample.norm.weight, encoders.camera.backbone.stages.2.downsample.norm.bias, encoders.camera.backbone.stages.2.downsample.reduction.weight, encoders.camera.backbone.stages.3.blocks.0.norm1.weight, encoders.camera.backbone.stages.3.blocks.0.norm1.bias, encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.proj.weight, encoders.camera.backbone.stages.3.blocks.0.attn.w_msa.proj.bias, encoders.camera.backbone.stages.3.blocks.0.norm2.weight, encoders.camera.backbone.stages.3.blocks.0.norm2.bias, encoders.camera.backbone.stages.3.blocks.0.ffn.layers.0.0.weight, encoders.camera.backbone.stages.3.blocks.0.ffn.layers.0.0.bias, encoders.camera.backbone.stages.3.blocks.0.ffn.layers.1.weight, encoders.camera.backbone.stages.3.blocks.0.ffn.layers.1.bias, encoders.camera.backbone.stages.3.blocks.1.norm1.weight, encoders.camera.backbone.stages.3.blocks.1.norm1.bias, encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table, encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.relative_position_index, encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight, encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias, encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.proj.weight, encoders.camera.backbone.stages.3.blocks.1.attn.w_msa.proj.bias, encoders.camera.backbone.stages.3.blocks.1.norm2.weight, encoders.camera.backbone.stages.3.blocks.1.norm2.bias, encoders.camera.backbone.stages.3.blocks.1.ffn.layers.0.0.weight, encoders.camera.backbone.stages.3.blocks.1.ffn.layers.0.0.bias, encoders.camera.backbone.stages.3.blocks.1.ffn.layers.1.weight, encoders.camera.backbone.stages.3.blocks.1.ffn.layers.1.bias, encoders.camera.backbone.norm1.weight, encoders.camera.backbone.norm1.bias, encoders.camera.backbone.norm2.weight, encoders.camera.backbone.norm2.bias, encoders.camera.backbone.norm3.weight, encoders.camera.backbone.norm3.bias, encoders.camera.neck.lateral_convs.0.conv.weight, encoders.camera.neck.lateral_convs.0.bn.weight, encoders.camera.neck.lateral_convs.0.bn.bias, encoders.camera.neck.lateral_convs.0.bn.running_mean, encoders.camera.neck.lateral_convs.0.bn.running_var, encoders.camera.neck.lateral_convs.1.conv.weight, encoders.camera.neck.lateral_convs.1.bn.weight, encoders.camera.neck.lateral_convs.1.bn.bias, encoders.camera.neck.lateral_convs.1.bn.running_mean, encoders.camera.neck.lateral_convs.1.bn.running_var, encoders.camera.neck.fpn_convs.0.conv.weight, encoders.camera.neck.fpn_convs.0.bn.weight, encoders.camera.neck.fpn_convs.0.bn.bias, encoders.camera.neck.fpn_convs.0.bn.running_mean, encoders.camera.neck.fpn_convs.0.bn.running_var, encoders.camera.neck.fpn_convs.1.conv.weight, encoders.camera.neck.fpn_convs.1.bn.weight, encoders.camera.neck.fpn_convs.1.bn.bias, encoders.camera.neck.fpn_convs.1.bn.running_mean, encoders.camera.neck.fpn_convs.1.bn.running_var, encoders.camera.vtransform.dx, encoders.camera.vtransform.bx, encoders.camera.vtransform.nx, encoders.camera.vtransform.frustum, encoders.camera.vtransform.dtransform.0.weight, encoders.camera.vtransform.dtransform.0.bias, encoders.camera.vtransform.dtransform.1.weight, encoders.camera.vtransform.dtransform.1.bias, encoders.camera.vtransform.dtransform.1.running_mean, encoders.camera.vtransform.dtransform.1.running_var, encoders.camera.vtransform.dtransform.3.weight, encoders.camera.vtransform.dtransform.3.bias, encoders.camera.vtransform.dtransform.4.weight, encoders.camera.vtransform.dtransform.4.bias, encoders.camera.vtransform.dtransform.4.running_mean, encoders.camera.vtransform.dtransform.4.running_var, encoders.camera.vtransform.dtransform.6.weight, encoders.camera.vtransform.dtransform.6.bias, encoders.camera.vtransform.dtransform.7.weight, encoders.camera.vtransform.dtransform.7.bias, encoders.camera.vtransform.dtransform.7.running_mean, encoders.camera.vtransform.dtransform.7.running_var, encoders.camera.vtransform.depthnet.0.weight, encoders.camera.vtransform.depthnet.0.bias, encoders.camera.vtransform.depthnet.1.weight, encoders.camera.vtransform.depthnet.1.bias, encoders.camera.vtransform.depthnet.1.running_mean, encoders.camera.vtransform.depthnet.1.running_var, encoders.camera.vtransform.depthnet.3.weight, encoders.camera.vtransform.depthnet.3.bias, encoders.camera.vtransform.depthnet.4.weight, encoders.camera.vtransform.depthnet.4.bias, encoders.camera.vtransform.depthnet.4.running_mean, encoders.camera.vtransform.depthnet.4.running_var, encoders.camera.vtransform.depthnet.6.weight, encoders.camera.vtransform.depthnet.6.bias, encoders.camera.vtransform.downsample.0.weight, encoders.camera.vtransform.downsample.1.weight, encoders.camera.vtransform.downsample.1.bias, encoders.camera.vtransform.downsample.1.running_mean, encoders.camera.vtransform.downsample.1.running_var, encoders.camera.vtransform.downsample.3.weight, encoders.camera.vtransform.downsample.4.weight, encoders.camera.vtransform.downsample.4.bias, encoders.camera.vtransform.downsample.4.running_mean, encoders.camera.vtransform.downsample.4.running_var, encoders.camera.vtransform.downsample.6.weight, encoders.camera.vtransform.downsample.7.weight, encoders.camera.vtransform.downsample.7.bias, encoders.camera.vtransform.downsample.7.running_mean, encoders.camera.vtransform.downsample.7.running_var, encoders.infra.backbone.patch_embed.projection.weight, encoders.infra.backbone.patch_embed.projection.bias, encoders.infra.backbone.patch_embed.norm.weight, encoders.infra.backbone.patch_embed.norm.bias, encoders.infra.backbone.stages.0.blocks.0.norm1.weight, encoders.infra.backbone.stages.0.blocks.0.norm1.bias, encoders.infra.backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.0.blocks.0.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.0.blocks.0.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.0.blocks.0.attn.w_msa.proj.weight, encoders.infra.backbone.stages.0.blocks.0.attn.w_msa.proj.bias, encoders.infra.backbone.stages.0.blocks.0.norm2.weight, encoders.infra.backbone.stages.0.blocks.0.norm2.bias, encoders.infra.backbone.stages.0.blocks.0.ffn.layers.0.0.weight, encoders.infra.backbone.stages.0.blocks.0.ffn.layers.0.0.bias, encoders.infra.backbone.stages.0.blocks.0.ffn.layers.1.weight, encoders.infra.backbone.stages.0.blocks.0.ffn.layers.1.bias, encoders.infra.backbone.stages.0.blocks.1.norm1.weight, encoders.infra.backbone.stages.0.blocks.1.norm1.bias, encoders.infra.backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.0.blocks.1.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.0.blocks.1.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.0.blocks.1.attn.w_msa.proj.weight, encoders.infra.backbone.stages.0.blocks.1.attn.w_msa.proj.bias, encoders.infra.backbone.stages.0.blocks.1.norm2.weight, encoders.infra.backbone.stages.0.blocks.1.norm2.bias, encoders.infra.backbone.stages.0.blocks.1.ffn.layers.0.0.weight, encoders.infra.backbone.stages.0.blocks.1.ffn.layers.0.0.bias, encoders.infra.backbone.stages.0.blocks.1.ffn.layers.1.weight, encoders.infra.backbone.stages.0.blocks.1.ffn.layers.1.bias, encoders.infra.backbone.stages.0.downsample.norm.weight, encoders.infra.backbone.stages.0.downsample.norm.bias, encoders.infra.backbone.stages.0.downsample.reduction.weight, encoders.infra.backbone.stages.1.blocks.0.norm1.weight, encoders.infra.backbone.stages.1.blocks.0.norm1.bias, encoders.infra.backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.1.blocks.0.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.1.blocks.0.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.1.blocks.0.attn.w_msa.proj.weight, encoders.infra.backbone.stages.1.blocks.0.attn.w_msa.proj.bias, encoders.infra.backbone.stages.1.blocks.0.norm2.weight, encoders.infra.backbone.stages.1.blocks.0.norm2.bias, encoders.infra.backbone.stages.1.blocks.0.ffn.layers.0.0.weight, encoders.infra.backbone.stages.1.blocks.0.ffn.layers.0.0.bias, encoders.infra.backbone.stages.1.blocks.0.ffn.layers.1.weight, encoders.infra.backbone.stages.1.blocks.0.ffn.layers.1.bias, encoders.infra.backbone.stages.1.blocks.1.norm1.weight, encoders.infra.backbone.stages.1.blocks.1.norm1.bias, encoders.infra.backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.1.blocks.1.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.1.blocks.1.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.1.blocks.1.attn.w_msa.proj.weight, encoders.infra.backbone.stages.1.blocks.1.attn.w_msa.proj.bias, encoders.infra.backbone.stages.1.blocks.1.norm2.weight, encoders.infra.backbone.stages.1.blocks.1.norm2.bias, encoders.infra.backbone.stages.1.blocks.1.ffn.layers.0.0.weight, encoders.infra.backbone.stages.1.blocks.1.ffn.layers.0.0.bias, encoders.infra.backbone.stages.1.blocks.1.ffn.layers.1.weight, encoders.infra.backbone.stages.1.blocks.1.ffn.layers.1.bias, encoders.infra.backbone.stages.1.downsample.norm.weight, encoders.infra.backbone.stages.1.downsample.norm.bias, encoders.infra.backbone.stages.1.downsample.reduction.weight, encoders.infra.backbone.stages.2.blocks.0.norm1.weight, encoders.infra.backbone.stages.2.blocks.0.norm1.bias, encoders.infra.backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.2.blocks.0.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.2.blocks.0.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.2.blocks.0.attn.w_msa.proj.weight, encoders.infra.backbone.stages.2.blocks.0.attn.w_msa.proj.bias, encoders.infra.backbone.stages.2.blocks.0.norm2.weight, encoders.infra.backbone.stages.2.blocks.0.norm2.bias, encoders.infra.backbone.stages.2.blocks.0.ffn.layers.0.0.weight, encoders.infra.backbone.stages.2.blocks.0.ffn.layers.0.0.bias, encoders.infra.backbone.stages.2.blocks.0.ffn.layers.1.weight, encoders.infra.backbone.stages.2.blocks.0.ffn.layers.1.bias, encoders.infra.backbone.stages.2.blocks.1.norm1.weight, encoders.infra.backbone.stages.2.blocks.1.norm1.bias, encoders.infra.backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.2.blocks.1.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.2.blocks.1.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.2.blocks.1.attn.w_msa.proj.weight, encoders.infra.backbone.stages.2.blocks.1.attn.w_msa.proj.bias, encoders.infra.backbone.stages.2.blocks.1.norm2.weight, encoders.infra.backbone.stages.2.blocks.1.norm2.bias, encoders.infra.backbone.stages.2.blocks.1.ffn.layers.0.0.weight, encoders.infra.backbone.stages.2.blocks.1.ffn.layers.0.0.bias, encoders.infra.backbone.stages.2.blocks.1.ffn.layers.1.weight, encoders.infra.backbone.stages.2.blocks.1.ffn.layers.1.bias, encoders.infra.backbone.stages.2.blocks.2.norm1.weight, encoders.infra.backbone.stages.2.blocks.2.norm1.bias, encoders.infra.backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.2.blocks.2.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.2.blocks.2.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.2.blocks.2.attn.w_msa.proj.weight, encoders.infra.backbone.stages.2.blocks.2.attn.w_msa.proj.bias, encoders.infra.backbone.stages.2.blocks.2.norm2.weight, encoders.infra.backbone.stages.2.blocks.2.norm2.bias, encoders.infra.backbone.stages.2.blocks.2.ffn.layers.0.0.weight, encoders.infra.backbone.stages.2.blocks.2.ffn.layers.0.0.bias, encoders.infra.backbone.stages.2.blocks.2.ffn.layers.1.weight, encoders.infra.backbone.stages.2.blocks.2.ffn.layers.1.bias, encoders.infra.backbone.stages.2.blocks.3.norm1.weight, encoders.infra.backbone.stages.2.blocks.3.norm1.bias, encoders.infra.backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.2.blocks.3.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.2.blocks.3.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.2.blocks.3.attn.w_msa.proj.weight, encoders.infra.backbone.stages.2.blocks.3.attn.w_msa.proj.bias, encoders.infra.backbone.stages.2.blocks.3.norm2.weight, encoders.infra.backbone.stages.2.blocks.3.norm2.bias, encoders.infra.backbone.stages.2.blocks.3.ffn.layers.0.0.weight, encoders.infra.backbone.stages.2.blocks.3.ffn.layers.0.0.bias, encoders.infra.backbone.stages.2.blocks.3.ffn.layers.1.weight, encoders.infra.backbone.stages.2.blocks.3.ffn.layers.1.bias, encoders.infra.backbone.stages.2.blocks.4.norm1.weight, encoders.infra.backbone.stages.2.blocks.4.norm1.bias, encoders.infra.backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.2.blocks.4.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.2.blocks.4.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.2.blocks.4.attn.w_msa.proj.weight, encoders.infra.backbone.stages.2.blocks.4.attn.w_msa.proj.bias, encoders.infra.backbone.stages.2.blocks.4.norm2.weight, encoders.infra.backbone.stages.2.blocks.4.norm2.bias, encoders.infra.backbone.stages.2.blocks.4.ffn.layers.0.0.weight, encoders.infra.backbone.stages.2.blocks.4.ffn.layers.0.0.bias, encoders.infra.backbone.stages.2.blocks.4.ffn.layers.1.weight, encoders.infra.backbone.stages.2.blocks.4.ffn.layers.1.bias, encoders.infra.backbone.stages.2.blocks.5.norm1.weight, encoders.infra.backbone.stages.2.blocks.5.norm1.bias, encoders.infra.backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.2.blocks.5.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.2.blocks.5.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.2.blocks.5.attn.w_msa.proj.weight, encoders.infra.backbone.stages.2.blocks.5.attn.w_msa.proj.bias, encoders.infra.backbone.stages.2.blocks.5.norm2.weight, encoders.infra.backbone.stages.2.blocks.5.norm2.bias, encoders.infra.backbone.stages.2.blocks.5.ffn.layers.0.0.weight, encoders.infra.backbone.stages.2.blocks.5.ffn.layers.0.0.bias, encoders.infra.backbone.stages.2.blocks.5.ffn.layers.1.weight, encoders.infra.backbone.stages.2.blocks.5.ffn.layers.1.bias, encoders.infra.backbone.stages.2.downsample.norm.weight, encoders.infra.backbone.stages.2.downsample.norm.bias, encoders.infra.backbone.stages.2.downsample.reduction.weight, encoders.infra.backbone.stages.3.blocks.0.norm1.weight, encoders.infra.backbone.stages.3.blocks.0.norm1.bias, encoders.infra.backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.3.blocks.0.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.3.blocks.0.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.3.blocks.0.attn.w_msa.proj.weight, encoders.infra.backbone.stages.3.blocks.0.attn.w_msa.proj.bias, encoders.infra.backbone.stages.3.blocks.0.norm2.weight, encoders.infra.backbone.stages.3.blocks.0.norm2.bias, encoders.infra.backbone.stages.3.blocks.0.ffn.layers.0.0.weight, encoders.infra.backbone.stages.3.blocks.0.ffn.layers.0.0.bias, encoders.infra.backbone.stages.3.blocks.0.ffn.layers.1.weight, encoders.infra.backbone.stages.3.blocks.0.ffn.layers.1.bias, encoders.infra.backbone.stages.3.blocks.1.norm1.weight, encoders.infra.backbone.stages.3.blocks.1.norm1.bias, encoders.infra.backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table, encoders.infra.backbone.stages.3.blocks.1.attn.w_msa.relative_position_index, encoders.infra.backbone.stages.3.blocks.1.attn.w_msa.qkv.weight, encoders.infra.backbone.stages.3.blocks.1.attn.w_msa.qkv.bias, encoders.infra.backbone.stages.3.blocks.1.attn.w_msa.proj.weight, encoders.infra.backbone.stages.3.blocks.1.attn.w_msa.proj.bias, encoders.infra.backbone.stages.3.blocks.1.norm2.weight, encoders.infra.backbone.stages.3.blocks.1.norm2.bias, encoders.infra.backbone.stages.3.blocks.1.ffn.layers.0.0.weight, encoders.infra.backbone.stages.3.blocks.1.ffn.layers.0.0.bias, encoders.infra.backbone.stages.3.blocks.1.ffn.layers.1.weight, encoders.infra.backbone.stages.3.blocks.1.ffn.layers.1.bias, encoders.infra.backbone.norm1.weight, encoders.infra.backbone.norm1.bias, encoders.infra.backbone.norm2.weight, encoders.infra.backbone.norm2.bias, encoders.infra.backbone.norm3.weight, encoders.infra.backbone.norm3.bias, encoders.infra.neck.lateral_convs.0.conv.weight, encoders.infra.neck.lateral_convs.0.bn.weight, encoders.infra.neck.lateral_convs.0.bn.bias, encoders.infra.neck.lateral_convs.0.bn.running_mean, encoders.infra.neck.lateral_convs.0.bn.running_var, encoders.infra.neck.lateral_convs.1.conv.weight, encoders.infra.neck.lateral_convs.1.bn.weight, encoders.infra.neck.lateral_convs.1.bn.bias, encoders.infra.neck.lateral_convs.1.bn.running_mean, encoders.infra.neck.lateral_convs.1.bn.running_var, encoders.infra.neck.fpn_convs.0.conv.weight, encoders.infra.neck.fpn_convs.0.bn.weight, encoders.infra.neck.fpn_convs.0.bn.bias, encoders.infra.neck.fpn_convs.0.bn.running_mean, encoders.infra.neck.fpn_convs.0.bn.running_var, encoders.infra.neck.fpn_convs.1.conv.weight, encoders.infra.neck.fpn_convs.1.bn.weight, encoders.infra.neck.fpn_convs.1.bn.bias, encoders.infra.neck.fpn_convs.1.bn.running_mean, encoders.infra.neck.fpn_convs.1.bn.running_var, encoders.infra.vtransform.dx, encoders.infra.vtransform.bx, encoders.infra.vtransform.nx, encoders.infra.vtransform.frustum, encoders.infra.vtransform.dtransform.0.weight, encoders.infra.vtransform.dtransform.0.bias, encoders.infra.vtransform.dtransform.1.weight, encoders.infra.vtransform.dtransform.1.bias, encoders.infra.vtransform.dtransform.1.running_mean, encoders.infra.vtransform.dtransform.1.running_var, encoders.infra.vtransform.dtransform.3.weight, encoders.infra.vtransform.dtransform.3.bias, encoders.infra.vtransform.dtransform.4.weight, encoders.infra.vtransform.dtransform.4.bias, encoders.infra.vtransform.dtransform.4.running_mean, encoders.infra.vtransform.dtransform.4.running_var, encoders.infra.vtransform.dtransform.6.weight, encoders.infra.vtransform.dtransform.6.bias, encoders.infra.vtransform.dtransform.7.weight, encoders.infra.vtransform.dtransform.7.bias, encoders.infra.vtransform.dtransform.7.running_mean, encoders.infra.vtransform.dtransform.7.running_var, encoders.infra.vtransform.depthnet.0.weight, encoders.infra.vtransform.depthnet.0.bias, encoders.infra.vtransform.depthnet.1.weight, encoders.infra.vtransform.depthnet.1.bias, encoders.infra.vtransform.depthnet.1.running_mean, encoders.infra.vtransform.depthnet.1.running_var, encoders.infra.vtransform.depthnet.3.weight, encoders.infra.vtransform.depthnet.3.bias, encoders.infra.vtransform.depthnet.4.weight, encoders.infra.vtransform.depthnet.4.bias, encoders.infra.vtransform.depthnet.4.running_mean, encoders.infra.vtransform.depthnet.4.running_var, encoders.infra.vtransform.depthnet.6.weight, encoders.infra.vtransform.depthnet.6.bias, encoders.infra.vtransform.downsample.0.weight, encoders.infra.vtransform.downsample.1.weight, encoders.infra.vtransform.downsample.1.bias, encoders.infra.vtransform.downsample.1.running_mean, encoders.infra.vtransform.downsample.1.running_var, encoders.infra.vtransform.downsample.3.weight, encoders.infra.vtransform.downsample.4.weight, encoders.infra.vtransform.downsample.4.bias, encoders.infra.vtransform.downsample.4.running_mean, encoders.infra.vtransform.downsample.4.running_var, encoders.infra.vtransform.downsample.6.weight, encoders.infra.vtransform.downsample.7.weight, encoders.infra.vtransform.downsample.7.bias, encoders.infra.vtransform.downsample.7.running_mean, encoders.infra.vtransform.downsample.7.running_var, fuser.0.weight, fuser.1.weight, fuser.1.bias, fuser.1.running_mean, fuser.1.running_var

2024-09-03 22:31:04,306 - mmdet3d - INFO - Start running, host: jmeng18@sg048, work_dir: /home/jmeng18/bev_fusion/bevfusion-beliv/train_result_infra_short_full
2024-09-03 22:31:04,307 - mmdet3d - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(HIGH        ) CyclicMomentumUpdaterHook          
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(HIGH        ) CyclicMomentumUpdaterHook          
(NORMAL      ) DistSamplerSeedHook                
(NORMAL      ) DistEvalHook                       
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(HIGH        ) CyclicMomentumUpdaterHook          
(NORMAL      ) DistEvalHook                       
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) DistEvalHook                       
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2024-09-03 22:31:04,307 - mmdet3d - INFO - workflow: [('train', 1)], max: 10 epochs
2024-09-03 22:31:04,309 - mmdet3d - INFO - Checkpoints will be saved to /home/jmeng18/bev_fusion/bevfusion-beliv/train_result_infra_short_full by HardDiskBackend.
2024-09-03 22:32:04,145 - mmdet3d - INFO - Epoch [1][50/8000]	lr: 7.973e-05, eta: 1 day, 2:31:11, time: 1.194, data_time: 0.081, memory: 6905, loss/object/loss_heatmap: 237.0069, loss/object/layer_-1_loss_cls: 1.3948, loss/object/layer_-1_loss_bbox: 11.4263, stats/object/matched_ious: 0.0086, loss: 249.8279, grad_norm: nan
2024-09-03 22:32:58,811 - mmdet3d - INFO - Epoch [1][100/8000]	lr: 9.307e-05, eta: 1 day, 1:23:03, time: 1.093, data_time: 0.014, memory: 6930, loss/object/loss_heatmap: 46.4410, loss/object/layer_-1_loss_cls: 0.7925, loss/object/layer_-1_loss_bbox: 5.4477, stats/object/matched_ious: 0.0224, loss: 52.6812, grad_norm: 524.8280
2024-09-03 22:33:53,905 - mmdet3d - INFO - Epoch [1][150/8000]	lr: 1.064e-04, eta: 1 day, 1:03:31, time: 1.102, data_time: 0.014, memory: 6930, loss/object/loss_heatmap: 6.3697, loss/object/layer_-1_loss_cls: 0.5812, loss/object/layer_-1_loss_bbox: 3.7658, stats/object/matched_ious: 0.0822, loss: 10.7167, grad_norm: 101.0648
2024-09-03 22:34:52,409 - mmdet3d - INFO - Epoch [1][200/8000]	lr: 1.197e-04, eta: 1 day, 1:15:59, time: 1.170, data_time: 0.086, memory: 6958, loss/object/loss_heatmap: 2.2991, loss/object/layer_-1_loss_cls: 0.3938, loss/object/layer_-1_loss_bbox: 1.8284, stats/object/matched_ious: 0.1733, loss: 4.5213, grad_norm: 30.9521
2024-09-03 22:35:52,579 - mmdet3d - INFO - Epoch [1][250/8000]	lr: 1.331e-04, eta: 1 day, 1:31:55, time: 1.203, data_time: 0.076, memory: 6958, loss/object/loss_heatmap: 1.5973, loss/object/layer_-1_loss_cls: 0.2519, loss/object/layer_-1_loss_bbox: 1.3819, stats/object/matched_ious: 0.2372, loss: 3.2311, grad_norm: 15.9698
2024-09-03 22:36:59,852 - mmdet3d - INFO - Epoch [1][300/8000]	lr: 1.464e-04, eta: 1 day, 2:13:40, time: 1.345, data_time: 0.282, memory: 6958, loss/object/loss_heatmap: 1.5755, loss/object/layer_-1_loss_cls: 0.2260, loss/object/layer_-1_loss_bbox: 1.5236, stats/object/matched_ious: 0.2103, loss: 3.3251, grad_norm: 20.4936
2024-09-03 22:38:14,397 - mmdet3d - INFO - Epoch [1][350/8000]	lr: 1.597e-04, eta: 1 day, 3:10:45, time: 1.491, data_time: 0.424, memory: 6958, loss/object/loss_heatmap: 1.2878, loss/object/layer_-1_loss_cls: 0.1784, loss/object/layer_-1_loss_bbox: 1.2630, stats/object/matched_ious: 0.2574, loss: 2.7292, grad_norm: 14.8135
2024-09-03 22:39:30,157 - mmdet3d - INFO - Epoch [1][400/8000]	lr: 1.731e-04, eta: 1 day, 3:57:16, time: 1.515, data_time: 0.404, memory: 6958, loss/object/loss_heatmap: 1.2554, loss/object/layer_-1_loss_cls: 0.1538, loss/object/layer_-1_loss_bbox: 1.2869, stats/object/matched_ious: 0.2749, loss: 2.6961, grad_norm: 12.1601
2024-09-03 22:40:50,243 - mmdet3d - INFO - Epoch [1][450/8000]	lr: 1.864e-04, eta: 1 day, 4:45:56, time: 1.602, data_time: 0.553, memory: 6958, loss/object/loss_heatmap: 1.2612, loss/object/layer_-1_loss_cls: 0.1480, loss/object/layer_-1_loss_bbox: 1.3562, stats/object/matched_ious: 0.2788, loss: 2.7653, grad_norm: 10.2695
2024-09-03 22:42:03,744 - mmdet3d - INFO - Epoch [1][500/8000]	lr: 1.997e-04, eta: 1 day, 5:07:08, time: 1.470, data_time: 0.382, memory: 6958, loss/object/loss_heatmap: 1.0603, loss/object/layer_-1_loss_cls: 0.1152, loss/object/layer_-1_loss_bbox: 1.2451, stats/object/matched_ious: 0.3069, loss: 2.4207, grad_norm: 10.1481
2024-09-03 22:43:07,234 - mmdet3d - INFO - Epoch [1][550/8000]	lr: 2.000e-04, eta: 1 day, 5:00:10, time: 1.270, data_time: 0.214, memory: 6958, loss/object/loss_heatmap: 1.1638, loss/object/layer_-1_loss_cls: 0.1485, loss/object/layer_-1_loss_bbox: 1.3005, stats/object/matched_ious: 0.3009, loss: 2.6129, grad_norm: 14.3482
2024-09-03 22:44:02,374 - mmdet3d - INFO - Epoch [1][600/8000]	lr: 2.000e-04, eta: 1 day, 4:35:45, time: 1.103, data_time: 0.052, memory: 6958, loss/object/loss_heatmap: 1.0668, loss/object/layer_-1_loss_cls: 0.1316, loss/object/layer_-1_loss_bbox: 1.2162, stats/object/matched_ious: 0.3068, loss: 2.4146, grad_norm: 9.7208
2024-09-03 22:45:02,995 - mmdet3d - INFO - Epoch [1][650/8000]	lr: 2.000e-04, eta: 1 day, 4:26:07, time: 1.212, data_time: 0.169, memory: 6958, loss/object/loss_heatmap: 1.1132, loss/object/layer_-1_loss_cls: 0.1258, loss/object/layer_-1_loss_bbox: 1.2769, stats/object/matched_ious: 0.3142, loss: 2.5159, grad_norm: 9.5822
2024-09-03 22:46:09,115 - mmdet3d - INFO - Epoch [1][700/8000]	lr: 2.000e-04, eta: 1 day, 4:28:06, time: 1.322, data_time: 0.240, memory: 6958, loss/object/loss_heatmap: 0.9766, loss/object/layer_-1_loss_cls: 0.1377, loss/object/layer_-1_loss_bbox: 1.0560, stats/object/matched_ious: 0.3352, loss: 2.1703, grad_norm: 9.8769
2024-09-03 22:47:03,125 - mmdet3d - INFO - Epoch [1][750/8000]	lr: 2.000e-04, eta: 1 day, 4:08:20, time: 1.080, data_time: 0.030, memory: 6958, loss/object/loss_heatmap: 1.0377, loss/object/layer_-1_loss_cls: 0.1262, loss/object/layer_-1_loss_bbox: 1.1485, stats/object/matched_ious: 0.3418, loss: 2.3124, grad_norm: 9.2293
2024-09-03 22:47:57,597 - mmdet3d - INFO - Epoch [1][800/8000]	lr: 2.000e-04, eta: 1 day, 3:51:41, time: 1.089, data_time: 0.014, memory: 6958, loss/object/loss_heatmap: 0.9534, loss/object/layer_-1_loss_cls: 0.1208, loss/object/layer_-1_loss_bbox: 1.0392, stats/object/matched_ious: 0.3555, loss: 2.1133, grad_norm: 7.8953
2024-09-03 22:48:49,536 - mmdet3d - INFO - Epoch [1][850/8000]	lr: 2.000e-04, eta: 1 day, 3:32:58, time: 1.039, data_time: 0.013, memory: 6958, loss/object/loss_heatmap: 0.9798, loss/object/layer_-1_loss_cls: 0.1151, loss/object/layer_-1_loss_bbox: 1.0851, stats/object/matched_ious: 0.3556, loss: 2.1801, grad_norm: 8.0564
2024-09-03 22:49:40,222 - mmdet3d - INFO - Epoch [1][900/8000]	lr: 2.000e-04, eta: 1 day, 3:14:23, time: 1.014, data_time: 0.012, memory: 6958, loss/object/loss_heatmap: 0.9663, loss/object/layer_-1_loss_cls: 0.1020, loss/object/layer_-1_loss_bbox: 1.2846, stats/object/matched_ious: 0.3506, loss: 2.3529, grad_norm: 7.7259
2024-09-03 22:50:33,154 - mmdet3d - INFO - Epoch [1][950/8000]	lr: 2.000e-04, eta: 1 day, 3:00:48, time: 1.059, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.9599, loss/object/layer_-1_loss_cls: 0.1186, loss/object/layer_-1_loss_bbox: 1.1531, stats/object/matched_ious: 0.3643, loss: 2.2316, grad_norm: 9.0346
2024-09-03 22:51:24,538 - mmdet3d - INFO - Epoch [1][1000/8000]	lr: 2.000e-04, eta: 1 day, 2:46:26, time: 1.028, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.9136, loss/object/layer_-1_loss_cls: 0.1006, loss/object/layer_-1_loss_bbox: 1.1882, stats/object/matched_ious: 0.3849, loss: 2.2024, grad_norm: 9.5115
2024-09-03 22:52:18,402 - mmdet3d - INFO - Epoch [1][1050/8000]	lr: 2.000e-04, eta: 1 day, 2:36:29, time: 1.077, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.8815, loss/object/layer_-1_loss_cls: 0.0957, loss/object/layer_-1_loss_bbox: 1.0847, stats/object/matched_ious: 0.4019, loss: 2.0619, grad_norm: 6.7177
2024-09-03 22:53:12,504 - mmdet3d - INFO - Epoch [1][1100/8000]	lr: 2.000e-04, eta: 1 day, 2:27:37, time: 1.082, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.9493, loss/object/layer_-1_loss_cls: 0.1237, loss/object/layer_-1_loss_bbox: 1.1057, stats/object/matched_ious: 0.4091, loss: 2.1787, grad_norm: 8.2936
2024-09-03 22:54:07,233 - mmdet3d - INFO - Epoch [1][1150/8000]	lr: 2.000e-04, eta: 1 day, 2:20:10, time: 1.095, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.9239, loss/object/layer_-1_loss_cls: 0.0983, loss/object/layer_-1_loss_bbox: 1.1303, stats/object/matched_ious: 0.3753, loss: 2.1526, grad_norm: 7.0119
2024-09-03 22:55:00,575 - mmdet3d - INFO - Epoch [1][1200/8000]	lr: 2.000e-04, eta: 1 day, 2:11:45, time: 1.067, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.9263, loss/object/layer_-1_loss_cls: 0.1000, loss/object/layer_-1_loss_bbox: 1.1460, stats/object/matched_ious: 0.3887, loss: 2.1723, grad_norm: 7.3152
2024-09-03 22:55:52,272 - mmdet3d - INFO - Epoch [1][1250/8000]	lr: 2.000e-04, eta: 1 day, 2:02:12, time: 1.034, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.8213, loss/object/layer_-1_loss_cls: 0.0893, loss/object/layer_-1_loss_bbox: 0.9787, stats/object/matched_ious: 0.4390, loss: 1.8893, grad_norm: 7.4137
2024-09-03 22:56:48,035 - mmdet3d - INFO - Epoch [1][1300/8000]	lr: 2.000e-04, eta: 1 day, 1:57:26, time: 1.115, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.8007, loss/object/layer_-1_loss_cls: 0.0959, loss/object/layer_-1_loss_bbox: 0.9643, stats/object/matched_ious: 0.4516, loss: 1.8609, grad_norm: 6.7997
2024-09-03 22:57:42,226 - mmdet3d - INFO - Epoch [1][1350/8000]	lr: 2.000e-04, eta: 1 day, 1:51:25, time: 1.084, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.8201, loss/object/layer_-1_loss_cls: 0.0978, loss/object/layer_-1_loss_bbox: 0.9318, stats/object/matched_ious: 0.4684, loss: 1.8496, grad_norm: 6.6239
2024-09-03 22:58:36,249 - mmdet3d - INFO - Epoch [1][1400/8000]	lr: 2.000e-04, eta: 1 day, 1:45:36, time: 1.080, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.7950, loss/object/layer_-1_loss_cls: 0.0904, loss/object/layer_-1_loss_bbox: 1.0434, stats/object/matched_ious: 0.4585, loss: 1.9288, grad_norm: 6.7859
2024-09-03 22:59:30,967 - mmdet3d - INFO - Epoch [1][1450/8000]	lr: 2.000e-04, eta: 1 day, 1:40:45, time: 1.094, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.8083, loss/object/layer_-1_loss_cls: 0.0950, loss/object/layer_-1_loss_bbox: 0.9358, stats/object/matched_ious: 0.4831, loss: 1.8391, grad_norm: 6.5626
2024-09-03 23:00:24,287 - mmdet3d - INFO - Epoch [1][1500/8000]	lr: 2.000e-04, eta: 1 day, 1:34:57, time: 1.066, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.8637, loss/object/layer_-1_loss_cls: 0.1058, loss/object/layer_-1_loss_bbox: 0.9810, stats/object/matched_ious: 0.4637, loss: 1.9506, grad_norm: 6.8766
2024-09-03 23:01:18,753 - mmdet3d - INFO - Epoch [1][1550/8000]	lr: 2.000e-04, eta: 1 day, 1:30:26, time: 1.089, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.7941, loss/object/layer_-1_loss_cls: 0.0955, loss/object/layer_-1_loss_bbox: 0.8914, stats/object/matched_ious: 0.4928, loss: 1.7809, grad_norm: 6.3230
2024-09-03 23:02:12,922 - mmdet3d - INFO - Epoch [1][1600/8000]	lr: 2.000e-04, eta: 1 day, 1:25:54, time: 1.083, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.7884, loss/object/layer_-1_loss_cls: 0.0913, loss/object/layer_-1_loss_bbox: 1.0006, stats/object/matched_ious: 0.4871, loss: 1.8803, grad_norm: 5.3155
2024-09-03 23:03:09,460 - mmdet3d - INFO - Epoch [1][1650/8000]	lr: 2.000e-04, eta: 1 day, 1:23:28, time: 1.131, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.8503, loss/object/layer_-1_loss_cls: 0.0919, loss/object/layer_-1_loss_bbox: 1.0717, stats/object/matched_ious: 0.4847, loss: 2.0139, grad_norm: 5.7377
2024-09-03 23:04:04,040 - mmdet3d - INFO - Epoch [1][1700/8000]	lr: 2.000e-04, eta: 1 day, 1:19:37, time: 1.092, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.7906, loss/object/layer_-1_loss_cls: 0.0941, loss/object/layer_-1_loss_bbox: 0.9523, stats/object/matched_ious: 0.5009, loss: 1.8370, grad_norm: 6.1572
2024-09-03 23:04:55,976 - mmdet3d - INFO - Epoch [1][1750/8000]	lr: 2.000e-04, eta: 1 day, 1:13:57, time: 1.039, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.7665, loss/object/layer_-1_loss_cls: 0.0898, loss/object/layer_-1_loss_bbox: 0.9543, stats/object/matched_ious: 0.5365, loss: 1.8107, grad_norm: 5.4995
2024-09-03 23:05:46,460 - mmdet3d - INFO - Epoch [1][1800/8000]	lr: 2.000e-04, eta: 1 day, 1:07:31, time: 1.010, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.7397, loss/object/layer_-1_loss_cls: 0.0831, loss/object/layer_-1_loss_bbox: 0.8675, stats/object/matched_ious: 0.5440, loss: 1.6903, grad_norm: 5.6140
2024-09-03 23:06:40,690 - mmdet3d - INFO - Epoch [1][1850/8000]	lr: 2.000e-04, eta: 1 day, 1:04:01, time: 1.085, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.7472, loss/object/layer_-1_loss_cls: 0.0849, loss/object/layer_-1_loss_bbox: 0.9407, stats/object/matched_ious: 0.5270, loss: 1.7728, grad_norm: 5.4969
2024-09-03 23:07:31,595 - mmdet3d - INFO - Epoch [1][1900/8000]	lr: 2.000e-04, eta: 1 day, 0:58:22, time: 1.018, data_time: 0.013, memory: 6969, loss/object/loss_heatmap: 0.6868, loss/object/layer_-1_loss_cls: 0.0870, loss/object/layer_-1_loss_bbox: 0.7543, stats/object/matched_ious: 0.5589, loss: 1.5281, grad_norm: 5.8042
2024-09-03 23:08:22,949 - mmdet3d - INFO - Epoch [1][1950/8000]	lr: 2.000e-04, eta: 1 day, 0:53:17, time: 1.027, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.7691, loss/object/layer_-1_loss_cls: 0.0878, loss/object/layer_-1_loss_bbox: 0.9440, stats/object/matched_ious: 0.5223, loss: 1.8009, grad_norm: 5.9835
2024-09-03 23:09:15,406 - mmdet3d - INFO - Epoch [1][2000/8000]	lr: 2.000e-04, eta: 1 day, 0:49:07, time: 1.049, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.8038, loss/object/layer_-1_loss_cls: 0.0939, loss/object/layer_-1_loss_bbox: 1.0846, stats/object/matched_ious: 0.5010, loss: 1.9823, grad_norm: 6.4426
2024-09-03 23:10:07,964 - mmdet3d - INFO - Epoch [1][2050/8000]	lr: 2.000e-04, eta: 1 day, 0:45:10, time: 1.051, data_time: 0.012, memory: 6969, loss/object/loss_heatmap: 0.7307, loss/object/layer_-1_loss_cls: 0.0901, loss/object/layer_-1_loss_bbox: 0.8117, stats/object/matched_ious: 0.5256, loss: 1.6325, grad_norm: 5.5234
2024-09-03 23:11:15,413 - mmdet3d - INFO - Epoch [1][2100/8000]	lr: 2.000e-04, eta: 1 day, 0:50:34, time: 1.349, data_time: 0.292, memory: 6969, loss/object/loss_heatmap: 0.7540, loss/object/layer_-1_loss_cls: 0.0912, loss/object/layer_-1_loss_bbox: 0.8524, stats/object/matched_ious: 0.5164, loss: 1.6976, grad_norm: 5.6148
2024-09-03 23:12:30,295 - mmdet3d - INFO - Epoch [1][2150/8000]	lr: 2.000e-04, eta: 1 day, 1:00:10, time: 1.498, data_time: 0.428, memory: 6969, loss/object/loss_heatmap: 0.6987, loss/object/layer_-1_loss_cls: 0.0848, loss/object/layer_-1_loss_bbox: 0.8421, stats/object/matched_ious: 0.5335, loss: 1.6256, grad_norm: 6.1683
2024-09-03 23:13:38,349 - mmdet3d - INFO - Epoch [1][2200/8000]	lr: 2.000e-04, eta: 1 day, 1:05:14, time: 1.361, data_time: 0.296, memory: 6969, loss/object/loss_heatmap: 0.7066, loss/object/layer_-1_loss_cls: 0.0789, loss/object/layer_-1_loss_bbox: 0.8275, stats/object/matched_ious: 0.5331, loss: 1.6130, grad_norm: 4.8090
2024-09-03 23:14:53,566 - mmdet3d - INFO - Epoch [1][2250/8000]	lr: 2.000e-04, eta: 1 day, 1:14:10, time: 1.504, data_time: 0.426, memory: 6969, loss/object/loss_heatmap: 0.7866, loss/object/layer_-1_loss_cls: 0.0916, loss/object/layer_-1_loss_bbox: 1.0565, stats/object/matched_ious: 0.5227, loss: 1.9347, grad_norm: 5.8440
2024-09-03 23:16:08,548 - mmdet3d - INFO - Epoch [1][2300/8000]	lr: 2.000e-04, eta: 1 day, 1:22:31, time: 1.500, data_time: 0.389, memory: 6969, loss/object/loss_heatmap: 0.7110, loss/object/layer_-1_loss_cls: 0.0937, loss/object/layer_-1_loss_bbox: 0.7722, stats/object/matched_ious: 0.5684, loss: 1.5769, grad_norm: 5.7855
2024-09-03 23:17:17,526 - mmdet3d - INFO - Epoch [1][2350/8000]	lr: 2.000e-04, eta: 1 day, 1:27:09, time: 1.380, data_time: 0.304, memory: 6969, loss/object/loss_heatmap: 0.7227, loss/object/layer_-1_loss_cls: 0.0874, loss/object/layer_-1_loss_bbox: 0.8499, stats/object/matched_ious: 0.5473, loss: 1.6599, grad_norm: 5.2097
2024-09-03 23:18:29,131 - mmdet3d - INFO - Epoch [1][2400/8000]	lr: 2.000e-04, eta: 1 day, 1:32:57, time: 1.432, data_time: 0.363, memory: 6969, loss/object/loss_heatmap: 0.7339, loss/object/layer_-1_loss_cls: 0.0912, loss/object/layer_-1_loss_bbox: 0.8278, stats/object/matched_ious: 0.5555, loss: 1.6529, grad_norm: 4.8114
2024-09-03 23:19:42,385 - mmdet3d - INFO - Epoch [1][2450/8000]	lr: 2.000e-04, eta: 1 day, 1:39:21, time: 1.465, data_time: 0.398, memory: 6969, loss/object/loss_heatmap: 0.6565, loss/object/layer_-1_loss_cls: 0.0806, loss/object/layer_-1_loss_bbox: 0.7907, stats/object/matched_ious: 0.5700, loss: 1.5278, grad_norm: 5.0875
2024-09-03 23:20:52,852 - mmdet3d - INFO - Epoch [1][2500/8000]	lr: 2.000e-04, eta: 1 day, 1:44:00, time: 1.409, data_time: 0.364, memory: 6969, loss/object/loss_heatmap: 0.6829, loss/object/layer_-1_loss_cls: 0.0870, loss/object/layer_-1_loss_bbox: 0.8055, stats/object/matched_ious: 0.5668, loss: 1.5754, grad_norm: 4.7862
2024-09-03 23:22:07,049 - mmdet3d - INFO - Epoch [1][2550/8000]	lr: 2.000e-04, eta: 1 day, 1:50:18, time: 1.484, data_time: 0.418, memory: 6969, loss/object/loss_heatmap: 0.6500, loss/object/layer_-1_loss_cls: 0.0782, loss/object/layer_-1_loss_bbox: 0.7140, stats/object/matched_ious: 0.5786, loss: 1.4422, grad_norm: 4.6330
2024-09-03 23:23:18,740 - mmdet3d - INFO - Epoch [1][2600/8000]	lr: 2.000e-04, eta: 1 day, 1:55:05, time: 1.434, data_time: 0.376, memory: 6969, loss/object/loss_heatmap: 0.6831, loss/object/layer_-1_loss_cls: 0.0828, loss/object/layer_-1_loss_bbox: 0.8185, stats/object/matched_ious: 0.5552, loss: 1.5844, grad_norm: 4.9653
2024-09-03 23:24:32,450 - mmdet3d - INFO - Epoch [1][2650/8000]	lr: 2.000e-04, eta: 1 day, 2:00:36, time: 1.474, data_time: 0.387, memory: 6969, loss/object/loss_heatmap: 0.6834, loss/object/layer_-1_loss_cls: 0.0857, loss/object/layer_-1_loss_bbox: 0.7557, stats/object/matched_ious: 0.5599, loss: 1.5248, grad_norm: 4.6813
2024-09-03 23:25:41,201 - mmdet3d - INFO - Epoch [1][2700/8000]	lr: 2.000e-04, eta: 1 day, 2:03:31, time: 1.375, data_time: 0.318, memory: 6969, loss/object/loss_heatmap: 0.6213, loss/object/layer_-1_loss_cls: 0.0796, loss/object/layer_-1_loss_bbox: 0.6935, stats/object/matched_ious: 0.5895, loss: 1.3944, grad_norm: nan
